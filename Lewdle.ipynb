{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05eb0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb1b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_unfor = \"BONER,FELCH,PUSSY,TAINT,SEMEN,DILDO,FARTS,CHODE,MINGE,GONAD,TWATS,SPUNK,QUEEF,PRICK,TITTY,CRAPS,BALLS,CUSSY,SPERM,BULGE,PANTS,PENIS,ARSES,CUNTS,KNOBS,BUSSY,FUCKS,JIZZY,WANKS,TURDS,SHITS,ASSES,COCKS,BUTTS,LOADS,CHOAD,BOOTY,QUIMS,DICKS,BOOBS,LEWDS,CUNTY,TESTE,FANNY,SHITE,DAMNS,FRICK,SHATS,TOOTS,POOPS,SHART,HUSSY,DONGS,SHAFT,GROIN,MOOBS,GROPE,LOINS,HORNY,JOBBY,NAKED,ERECT,RANDY,POUND,NUDES,HINEY,BREED,COOCH,PORNO,MOANS,CUMMY,LABIA,PUBES,BONED,VULVA,CLITS,BUSTY,FECES,TEATS,FECAL,SCREW,BOINK,GOOCH,MOIST,SPANK,WHIPS,ROPES,LUBED,KINKY,BOOBY,FUCKY,CREAM,BLOWS,SNOGS,WILLY,TRUMP,MILFS,DILFS,GILFS,PISSY,URINE,POOPY,FISTS,VEINY,THROB,SWING,THICK,RIMMY,SMASH,TWERK,ENEMA,WANGS,ORGAN,FILTH,CHOKE,MOMMY,DADDY,MUNCH,HANDY,PUBIC,THIGH,FLAPS,WENIS,HOOCH,WENCH,SKEET,RYCOP,BOOBA,POTTY,NARDS,GAPED,FUCKO,PERKY,KEGEL,BIMBO,CRACK,TWUNT,BANGS,CUNNY,SHAGS,MUFFS,HIMBO,VIXEN,GROOL,TWINK,CUFFS,CHEEK,DROOL,PERVY,LOVER,DOINK,BUTCH,BITCH,WHORE,SLUTS,THICC,GIRTH,ROOTS,CHUFF,FLUFF,SOUND,CRANK,FUGLY,TIGHT,LOOSE,SLURP,LUSTY,LUSTS,SOAPY,CUCKS,SKANK,STANK,FREAK,GUSHY,STRIP,TEASE,QUEER,TRICK,SISSY,FURRY,PUFFY,BAWDY,SPICY,CUMSY,PLOPS,VADGE,PORKS,THONG,GIMPS,ROUGH,BOUND,CHAIN,DIRTY,VINYL,HOLES,WAXED,SLAVE,BITES,LICKS,PLUGS,HAIRY,BUSHY,BEARS,SLAPS,LATEX,CAGED,CAGES,PANTY,FLASH,BULLS,CURVY,SPITS,MILKS,MILKY,DOGGY,BRATS,EROGE,NASTY,DIRTY\"\n",
    "target_unfor = \"BONER,FELCH,PUSSY,TAINT,SEMEN,DILDO,FARTS,CHODE,MINGE,GONAD,TWATS,SPUNK,QUEEF,GAPED,PRICK,BALLS,CUSSY,SPERM,BULGE,PENIS,ARSES,CUNTS,KNOBS,BUSSY,FUCKS,JIZZY,SHITS,ASSES,COCKS,BUTTS,LOADS,CHOAD,BOOTY,QUIMS,DICKS,BOOBS,LEWDS,CUNTY,FANNY,SHITE,FRICK,POOPS,SHART,WANKS,HUSSY,DONGS,SHAFT,GROIN,MOOBS,GROPE,LOINS,HORNY,JOBBY,NAKED,TITTY,ERECT,RANDY,POUND,NUDES,HINEY,BREED,COOCH,PORNO,MOANS,CUMMY,LABIA,TURDS,PUBES,DADDY,BONED,VULVA,CLITS,BUSTY,FECES,TEATS,FECAL,SCREW,BOINK,GOOCH,MOIST,SPANK,LUBED,KINKY,BOOBY,CREAM,BLOWS,SNOGS,WILLY,TRUMP,MILFS,DILFS,GILFS,PISSY,URINE,POOPY,FISTS,VEINY,THROB,SWING,THICK,RIMMY,SMASH,TWERK,ENEMA,WANGS,ORGAN,FILTH,CHOKE,MOMMY,CRAPS,HANDY,PUBIC,FLAPS,WENIS,HOOCH,WENCH,SKEET,POTTY,NARDS,PERKY,CRACK,TWUNT,BANGS,CUNNY,SHAGS,MUFFS,HIMBO,VIXEN,GROOL,TWINK,PERVY,DOINK,THICC,GIRTH,DOGGY,NASTY,DIRTY\"\n",
    "word_list = words_unfor.split(',')\n",
    "word_target = target_unfor.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc3607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array = np.zeros((len(word_list), 5))\n",
    "target_array = np.zeros((len(word_target), 5))\n",
    "\n",
    "for i in range(len(word_list)):\n",
    "    word_array[i] = [ord(character)-96 for character in word_list[i]]\n",
    "    \n",
    "for i in range(len(word_target)):\n",
    "    target_array[i] = [ord(character)-96 for character in word_target[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a0e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [2, 6, 18, 54, 162, 1, 3, 9, 27, 81, 1, 3, 9, 27, 81, 1, 3, 9, 27, 81, 1, 3, 9, 27, 81]\n",
    "def simi_scores(target_vec, word_array):\n",
    "    target_vec2 = np.concatenate((target_vec, target_vec[1:], target_vec[:1], target_vec[2:], \n",
    "                                  target_vec[:2], target_vec[3:], target_vec[:3], target_vec[4:], target_vec[:4]))\n",
    "    word_array2 = np.concatenate((word_array, word_array, word_array, word_array, word_array), axis = 1)\n",
    "    word_array2 = target_vec2 - word_array2\n",
    "    word_array2 = (word_array2 == 0)\n",
    "    simi_scores = word_array2@scoring\n",
    "    return simi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce34b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_array = np.zeros((len(word_target), len(word_list)))\n",
    "for i in range(len(word_target)):\n",
    "    similarity_array[i] = simi_scores(target_array[i], word_array)\n",
    "simi_list = [similarity_array[i].tolist() for i in range(len(similarity_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e0a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(word):\n",
    "    return word_list.index(word)\n",
    "\n",
    "def find_target(target):\n",
    "    return word_target.index(target)\n",
    "\n",
    "def filter(test_word, score, original):\n",
    "    word = find_word(test_word)\n",
    "    return [i for i in original if similarity_array[i, word] == score]\n",
    "\n",
    "def display(list):\n",
    "    words = []\n",
    "    for i in list:\n",
    "        words.append(word_target[i])\n",
    "    return words\n",
    "\n",
    "def filter_by_matrix(word, score, original):\n",
    "    simi_list = [vec for vec in original if vec[word] == score]\n",
    "    return simi_list\n",
    "\n",
    "def return_filtered_length(word, score, original):\n",
    "    return len([1 for vec in original if vec[word] == score])\n",
    "\n",
    "def length_heuristic(length):\n",
    "    if length < 2:\n",
    "        return 0\n",
    "    elif length < 10:\n",
    "        return length\n",
    "    elif length < 20:\n",
    "        return length*1.3 #(2-check_for_split(simi_list))\n",
    "    else: \n",
    "        return length*1.7\n",
    "    \n",
    "def find_word_from_vec(vec):\n",
    "    return find_word(word_target[simi_list.index(vec)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41297256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_split(simi_list):\n",
    "    if len(simi_list) == 0:\n",
    "        return 1\n",
    "    array = np.transpose(np.array(simi_list))\n",
    "    lengths = [len(np.unique(array[i])) for i in range(len(word_list))]\n",
    "    return max(lengths)/len(simi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3612f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = len(word_list)\n",
    "def best_next_word_matrix(original):\n",
    "    best_word = \"None!\"\n",
    "    \n",
    "    if len(original) == 0:\n",
    "        return best_word, 0\n",
    "    \n",
    "    matching_words = [find_word_from_vec(vec) for vec in original]\n",
    "    matching_scores = np.array([i in matching_words for i in range(check)])\n",
    "    scores = [sum([length_heuristic(return_filtered_length(i, j, original)) for j in range(243)]) for i in range(check)]\n",
    "    scores_np = np.array(scores) - matching_scores\n",
    "    #best = np.argmin(scores_np)\n",
    "    \n",
    "    ind = np.argpartition(scores_np, 9)[:10]\n",
    "    best_score = 10000\n",
    "    #scores2 = []\n",
    "    for x in ind:\n",
    "        score = 0\n",
    "        for j in range(243):\n",
    "            simi_list = filter_by_matrix(x, j, original)\n",
    "            length = len(simi_list)\n",
    "            if length < 2:\n",
    "                continue\n",
    "            elif length < 6:\n",
    "                score += length\n",
    "            else:\n",
    "                score += (3 - 2*check_for_split(simi_list))*length\n",
    "        if x in matching_words:\n",
    "            score -= 1\n",
    "        #scores2.append(score)\n",
    "        if score < best_score:\n",
    "            best_word = word_list[x]\n",
    "            best_score = score\n",
    "\n",
    "    return best_word, 1 + best_score/len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9087a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_next_word_matrix2(original):\n",
    "    best_score = 10000\n",
    "    best_word = \"None!\"\n",
    "    best_chance = 0\n",
    "    original_np = np.transpose(np.array(original))\n",
    "    if len(original) == 0:\n",
    "        return best_word, 0\n",
    "    for i in range(1000): #range(len(word_list)):\n",
    "        chance = 0\n",
    "        lengths = [length_heuristic(return_filtered_length(i, j, original_np)) for j in range(243)]\n",
    "        score = sum(lengths)\n",
    "    \n",
    "        if score < best_score:\n",
    "            best_word = word_list[i]\n",
    "            best_score = score\n",
    "\n",
    "    return best_word, 1 + best_score/len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1474ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi_from_colour(list):\n",
    "    sum = 0\n",
    "    for i in range(5):\n",
    "        sum += list[i]*(3**i)\n",
    "    return sum\n",
    "def colour_from_simi(simi):\n",
    "    colour = [0,0,0,0,0]\n",
    "    for i in [4,3,2,1,0]:\n",
    "        x = simi//3**i\n",
    "        simi -= x*3**i\n",
    "        colour[i] = x\n",
    "    return colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "676a91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_first_word(word):\n",
    "    results = []\n",
    "    total_score = 0\n",
    "    y = find_word(word)\n",
    "    for i in range(243):\n",
    "        x = filter_by_matrix(y, i, simi_list)\n",
    "        word_, score = best_next_word_matrix(x)\n",
    "        total_score += score*len(x)\n",
    "        results.append((i, word_, score))\n",
    "    \n",
    "    return 1 + total_score/len(word_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fbdca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_first_word_verbose(word):\n",
    "    results = []\n",
    "    total_score = 0\n",
    "    y = find_word(word)\n",
    "    for i in range(243):\n",
    "        x = filter_by_matrix(y, i, simi_list)\n",
    "        word_, score = best_next_word_matrix(x)\n",
    "        total_score += score*len(x)\n",
    "        results.append((i, word_, score))\n",
    "        \n",
    "    try: \n",
    "        os.remove(f'{word}.txt')\n",
    "    except: \n",
    "        None\n",
    "        \n",
    "    with open(f'{word}.txt', 'w') as fp:\n",
    "        for (i, word_, score) in results:\n",
    "            fp.write(f\"{colour_from_simi(i)}: {word_}, {score}\\n\")\n",
    "        fp.close()\n",
    "    \n",
    "    return 1 + total_score/len(word_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74dc333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOINS:1.72992700729927\n",
      "PENIS:1.7226277372262775\n",
      "BITES:1.7226277372262775\n",
      "CUNTS:1.7226277372262775\n",
      "PANTS:1.7372262773722627\n",
      "Time taken:259.924453496933\n"
     ]
    }
   ],
   "source": [
    "time1 = time()\n",
    "scores = []\n",
    "for word in word_list:    \n",
    "    score = test_first_word(word)\n",
    "    scores.append(score)\n",
    "\n",
    "scores_np = np.array(scores)\n",
    "ind = np.argpartition(scores_np, 4)[:5]\n",
    "for x in ind:\n",
    "    score = test_first_word_verbose(word_list[x])\n",
    "    print(f\"{word_list[x]}:{score}\")\n",
    "\n",
    "print(f\"Time taken:{time() - time1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18b0bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOINS:1.72992700729927\n",
      "PENIS:1.7226277372262775\n",
      "BITES:1.7226277372262775\n",
      "CUNTS:1.7226277372262775\n",
      "PANTS:1.7372262773722627\n"
     ]
    }
   ],
   "source": [
    "words = ['LOINS', 'PENIS', 'BITES', 'CUNTS', 'PANTS']\n",
    "for word in words:\n",
    "    score = test_first_word_verbose(word)\n",
    "    print(f\"{word}:{score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "simi_from_colour([1,1,0,0,2])\n",
    "#colour_from_simi(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "211226ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRICK', 'FRICK']\n"
     ]
    }
   ],
   "source": [
    "x = filter('BONER', 81, range(len(word_target)))\n",
    "y = filter('SHITE', 18, x)\n",
    "#z = filter('flong', 84, y)\n",
    "print(display(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ad15fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = filter_by_matrix(find_word('BONER'), 81, simi_list)\n",
    "y = filter_by_matrix(find_word('SHITE'), 18, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a52ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FRICK', 0.5)\n"
     ]
    }
   ],
   "source": [
    "print(best_next_word_matrix(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
